{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\programdata\\miniconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\miniconda3\\lib\\site-packages (from lightgbm) (0.20.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniconda3\\lib\\site-packages (from lightgbm) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (from lightgbm) (1.15.4)\n",
      "Requirement already satisfied: eli5 in c:\\programdata\\miniconda3\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: graphviz in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (0.10.1)\n",
      "Requirement already satisfied: typing in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (3.6.6)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (0.8.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (1.11.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (1.15.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (1.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (2.10)\n",
      "Requirement already satisfied: attrs>16.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from eli5) (19.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2->eli5) (1.1.0)\n",
      "Requirement already satisfied: shap in c:\\programdata\\miniconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (4.28.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (0.20.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (1.15.4)\n",
      "Requirement already satisfied: ipython in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (7.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (1.1.0)\n",
      "Requirement already satisfied: scikit-image in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (0.15.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (from shap) (0.24.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->shap) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->shap) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->shap) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from matplotlib->shap) (0.10.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (0.13.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (2.0.7)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (40.6.2)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (2.3.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (0.4.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (4.3.2)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (4.3.0)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython->shap) (0.1.0)\n",
      "Requirement already satisfied: imageio>=2.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-image->shap) (2.5.0)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-image->shap) (2.3)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-image->shap) (1.0.3)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-image->shap) (5.3.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->shap) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->shap) (1.11.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jedi>=0.10->ipython->shap) (0.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\miniconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->shap) (0.1.7)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\miniconda3\\lib\\site-packages (from traitlets>=4.2->ipython->shap) (0.2.0)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\miniconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\miniconda3\\lib\\site-packages (from sklearn) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (1.15.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (1.15.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\lib\\site-packages (4.28.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install eli5\n",
    "!pip install shap\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import auc, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADMISSIONS.csv', 'CALLOUT.csv', 'CAREGIVERS.csv', 'CPTEVENTS.csv', 'DATETIMEEVENTS.csv', 'DIAGNOSES_ICD.csv', 'DRGCODES.csv', 'D_CPT.csv', 'D_ICD_DIAGNOSES.csv', 'D_ICD_PROCEDURES.csv', 'D_ITEMS.csv', 'D_LABITEMS.csv', 'ICUSTAYS.csv', 'INPUTEVENTS_CV.csv', 'INPUTEVENTS_MV.csv', 'LABEVENTS.csv', 'MICROBIOLOGYEVENTS.csv', 'OUTPUTEVENTS.csv', 'PATIENTS.csv', 'PRESCRIPTIONS.csv', 'PROCEDUREEVENTS_MV.csv', 'PROCEDURES_ICD.csv', 'SERVICES.csv', 'TRANSFERS.csv']\n",
      "ADMISSIONS.csv\n",
      "58976\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS', 'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA']\n",
      "CALLOUT.csv\n",
      "34499\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SUBMIT_WARDID', 'SUBMIT_CAREUNIT', 'CURR_WARDID', 'CURR_CAREUNIT', 'CALLOUT_WARDID', 'CALLOUT_SERVICE', 'REQUEST_TELE', 'REQUEST_RESP', 'REQUEST_CDIFF', 'REQUEST_MRSA', 'REQUEST_VRE', 'CALLOUT_STATUS', 'CALLOUT_OUTCOME', 'DISCHARGE_WARDID', 'ACKNOWLEDGE_STATUS', 'CREATETIME', 'UPDATETIME', 'ACKNOWLEDGETIME', 'OUTCOMETIME', 'FIRSTRESERVATIONTIME', 'CURRENTRESERVATIONTIME']\n",
      "CAREGIVERS.csv\n",
      "7567\n",
      "['ROW_ID', 'CGID', 'LABEL', 'DESCRIPTION']\n",
      "CPTEVENTS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (4,5,7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573146\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'COSTCENTER', 'CHARTDATE', 'CPT_CD', 'CPT_NUMBER', 'CPT_SUFFIX', 'TICKET_ID_SEQ', 'SECTIONHEADER', 'SUBSECTIONHEADER', 'DESCRIPTION']\n",
      "DATETIMEEVENTS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4485937\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'ITEMID', 'CHARTTIME', 'STORETIME', 'CGID', 'VALUE', 'VALUEUOM', 'WARNING', 'ERROR', 'RESULTSTATUS', 'STOPPED']\n",
      "DIAGNOSES_ICD.csv\n",
      "651047\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "DRGCODES.csv\n",
      "125557\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'DRG_TYPE', 'DRG_CODE', 'DESCRIPTION', 'DRG_SEVERITY', 'DRG_MORTALITY']\n",
      "D_CPT.csv\n",
      "134\n",
      "['ROW_ID', 'CATEGORY', 'SECTIONRANGE', 'SECTIONHEADER', 'SUBSECTIONRANGE', 'SUBSECTIONHEADER', 'CODESUFFIX', 'MINCODEINSUBSECTION', 'MAXCODEINSUBSECTION']\n",
      "D_ICD_DIAGNOSES.csv\n",
      "14567\n",
      "['ROW_ID', 'ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE']\n",
      "D_ICD_PROCEDURES.csv\n",
      "3882\n",
      "['ROW_ID', 'ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE']\n",
      "D_ITEMS.csv\n",
      "12487\n",
      "['ROW_ID', 'ITEMID', 'LABEL', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CATEGORY', 'UNITNAME', 'PARAM_TYPE', 'CONCEPTID']\n",
      "D_LABITEMS.csv\n",
      "753\n",
      "['ROW_ID', 'ITEMID', 'LABEL', 'FLUID', 'CATEGORY', 'LOINC_CODE']\n",
      "ICUSTAYS.csv\n",
      "61532\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'DBSOURCE', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'FIRST_WARDID', 'LAST_WARDID', 'INTIME', 'OUTTIME', 'LOS']\n",
      "INPUTEVENTS_CV.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (7,9,17,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17527935\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'AMOUNT', 'AMOUNTUOM', 'RATE', 'RATEUOM', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'STOPPED', 'NEWBOTTLE', 'ORIGINALAMOUNT', 'ORIGINALAMOUNTUOM', 'ORIGINALROUTE', 'ORIGINALRATE', 'ORIGINALRATEUOM', 'ORIGINALSITE']\n",
      "INPUTEVENTS_MV.csv\n",
      "3618991\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTTIME', 'ENDTIME', 'ITEMID', 'AMOUNT', 'AMOUNTUOM', 'RATE', 'RATEUOM', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'ORDERCATEGORYNAME', 'SECONDARYORDERCATEGORYNAME', 'ORDERCOMPONENTTYPEDESCRIPTION', 'ORDERCATEGORYDESCRIPTION', 'PATIENTWEIGHT', 'TOTALAMOUNT', 'TOTALAMOUNTUOM', 'ISOPENBAG', 'CONTINUEINNEXTDEPT', 'CANCELREASON', 'STATUSDESCRIPTION', 'COMMENTS_EDITEDBY', 'COMMENTS_CANCELEDBY', 'COMMENTS_DATE', 'ORIGINALAMOUNT', 'ORIGINALRATE']\n",
      "LABEVENTS.csv\n",
      "27854055\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ITEMID', 'CHARTTIME', 'VALUE', 'VALUENUM', 'VALUEUOM', 'FLAG']\n",
      "MICROBIOLOGYEVENTS.csv\n",
      "631726\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'SPEC_ITEMID', 'SPEC_TYPE_DESC', 'ORG_ITEMID', 'ORG_NAME', 'ISOLATE_NUM', 'AB_ITEMID', 'AB_NAME', 'DILUTION_TEXT', 'DILUTION_COMPARISON', 'DILUTION_VALUE', 'INTERPRETATION']\n",
      "OUTPUTEVENTS.csv\n",
      "4349218\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM', 'STORETIME', 'CGID', 'STOPPED', 'NEWBOTTLE', 'ISERROR']\n",
      "PATIENTS.csv\n",
      "46520\n",
      "['ROW_ID', 'SUBJECT_ID', 'GENDER', 'DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN', 'EXPIRE_FLAG']\n",
      "PRESCRIPTIONS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156450\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE', 'ENDDATE', 'DRUG_TYPE', 'DRUG', 'DRUG_NAME_POE', 'DRUG_NAME_GENERIC', 'FORMULARY_DRUG_CD', 'GSN', 'NDC', 'PROD_STRENGTH', 'DOSE_VAL_RX', 'DOSE_UNIT_RX', 'FORM_VAL_DISP', 'FORM_UNIT_DISP', 'ROUTE']\n",
      "PROCEDUREEVENTS_MV.csv\n",
      "258066\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTTIME', 'ENDTIME', 'ITEMID', 'VALUE', 'VALUEUOM', 'LOCATION', 'LOCATIONCATEGORY', 'STORETIME', 'CGID', 'ORDERID', 'LINKORDERID', 'ORDERCATEGORYNAME', 'SECONDARYORDERCATEGORYNAME', 'ORDERCATEGORYDESCRIPTION', 'ISOPENBAG', 'CONTINUEINNEXTDEPT', 'CANCELREASON', 'STATUSDESCRIPTION', 'COMMENTS_EDITEDBY', 'COMMENTS_CANCELEDBY', 'COMMENTS_DATE']\n",
      "PROCEDURES_ICD.csv\n",
      "240095\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "SERVICES.csv\n",
      "73343\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'TRANSFERTIME', 'PREV_SERVICE', 'CURR_SERVICE']\n",
      "TRANSFERS.csv\n",
      "261897\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'DBSOURCE', 'EVENTTYPE', 'PREV_CAREUNIT', 'CURR_CAREUNIT', 'PREV_WARDID', 'CURR_WARDID', 'INTIME', 'OUTTIME', 'LOS']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/'\n",
    "csvs = [x for x in os.listdir(data_dir) if x.endswith('.csv')]\n",
    "print(csvs)\n",
    "for csv in csvs:\n",
    "    print(csv)\n",
    "    df = pd.read_csv(os.path.join(data_dir, csv))\n",
    "    print(len(df))\n",
    "    print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5307071688555796\n",
      "125557\n",
      "46511\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'DRGCODES.csv'))\n",
    "print(np.count_nonzero(~np.isnan(df['DRG_MORTALITY'].values)) / len(df['DRG_MORTALITY'].values))\n",
    "print(len(df['SUBJECT_ID'].values))\n",
    "print(len(np.unique(df['SUBJECT_ID'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    \"\"\"Abstract base model class.\"\"\"\n",
    "\n",
    "    def __init__(self, k_folds, tuning_metric, num_trials, confidence_level):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        self.k_folds = k_folds\n",
    "        self.tuning_metric = tuning_metric\n",
    "        self.confidence_level = confidence_level\n",
    "        self.num_trials = num_trials\n",
    "\n",
    "        self.eval_fn = sklearn.metrics.auc\n",
    "        self.model = None\n",
    "        self.model_fn = None\n",
    "        self.best_params = None\n",
    "        self.fixed_parameters = {}\n",
    "        self.tuning_parameters = {}\n",
    "\n",
    "    def train(self, x, y, feature_names=[]):\n",
    "        \"\"\"Train a model on the inputs and labels provided.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): Input data to train on\n",
    "            y (numpy.ndarray): Labels corresponding to the input data\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Input shape is {x.shape}')\n",
    "\n",
    "        initial_parameters = self.fixed_parameters\n",
    "        initial_parameters.update(\n",
    "            {param: val[0]\n",
    "                for param, val in self.tuning_parameters[0].items()})\n",
    "\n",
    "        # Handle single output model instantiation\n",
    "        self.model = self.model_fn(**initial_parameters)\n",
    "\n",
    "        scoring_fn = make_scorer(\n",
    "            self.eval_fn,\n",
    "            greater_is_better=True)\n",
    "\n",
    "        best_score = float('-inf')\n",
    "        best_model = self.model\n",
    "        folds = list(StratifiedKFold(self.k_folds).split(x, y))\n",
    "\n",
    "        for i in tqdm(range(len(self.tuning_parameters))):\n",
    "            if logger is not None:\n",
    "                logger.log(f\"Tuning step {i+1}:\", self.tuning_parameters[i])\n",
    "\n",
    "            # Perform a grid search over the parameters specified\n",
    "            # in self.tuning_parameters[i].\n",
    "            # Set verbose to 2 for printing each step of the grid search.\n",
    "            # Set n_jobs to -1 to use all processors.\n",
    "            grid_searcher = GridSearchCV(estimator=self.model,\n",
    "                 param_grid=self.tuning_parameters[i],\n",
    "                 scoring=scoring_fn,\n",
    "                 iid=False,\n",
    "                 cv=folds,\n",
    "                 refit=False,\n",
    "                 n_jobs=-1)\n",
    "            grid_searcher.fit(x, y)\n",
    "\n",
    "            # Record the entire CSV regardless for future reference\n",
    "            results = grid_searcher.cv_results_\n",
    "            df = pd.DataFrame(results)\n",
    "\n",
    "            if grid_searcher.best_score_ > best_score:\n",
    "\n",
    "                best_params = grid_searcher.best_params_\n",
    "                best_score = grid_searcher.best_score_\n",
    "\n",
    "                fixed_parameters = self.fixed_parameters\n",
    "                fixed_parameters.update({param: val\n",
    "                                         for param, val in best_params.items()})\n",
    "                best_model = self.model_fn(**fixed_parameters)\n",
    "\n",
    "\n",
    "        self.model = best_model\n",
    "\n",
    "        # K-fold cross validation.\n",
    "        sample_ids = np.zeros(x.shape[0])\n",
    "        ground_truth = np.zeros(x.shape[0])\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        shap_values = np.zeros(x.shape)\n",
    "        shap_interaction_values = np.zeros(shape=(x.shape[0], x.shape[1], x.shape[1]))\n",
    "        expected_values = []\n",
    "        for train_ids, test_ids in folds:\n",
    "            x_train = x[train_ids]\n",
    "            y_train = y[train_ids]\n",
    "            self.model.fit(x_train, y_train)\n",
    "\n",
    "            explanation = eli5.explain_weights_lightgbm(\n",
    "                self.model,\n",
    "                feature_names=feature_names,\n",
    "                top=25)\n",
    "            logger.log(eli5.format_as_text(explanation))\n",
    "\n",
    "            x_test = x[test_ids]\n",
    "            y_test = y[test_ids]\n",
    "            y_pred = self.model.predict(x_test)\n",
    "\n",
    "            sample_ids[test_ids] = test_ids\n",
    "            ground_truth[test_ids] = y_test\n",
    "            predictions[test_ids] = y_pred\n",
    "\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values[test_ids] = explainer.shap_values(x_test)\n",
    "            shap_interaction_values[test_ids] = explainer.shap_interaction_values(x_test)\n",
    "            expected_values.append(explainer.expected_value)\n",
    "\n",
    "        shap_dir = logger.outputs_dir / 'shap'\n",
    "        shap_dir.mkdir()\n",
    "\n",
    "        expected_value = np.mean(expected_values)\n",
    "        logger.log_output(expected_values, 'shap/expected_values')\n",
    "        logger.log_output(shap_values, 'shap/shap_values')\n",
    "        logger.log_output(shap_interaction_values,\n",
    "                          'shap/shap_interaction_values')\n",
    "        logger.log_output(x, 'shap/x')\n",
    "        logger.log_output(feature_names, 'shap/feature_names')\n",
    "\n",
    "        record = np.hstack((sample_ids.reshape(-1, 1),\n",
    "                            ground_truth.reshape(-1, 1),\n",
    "                            predictions.reshape(-1, 1)))\n",
    "        print(f'Record shape is {record.shape}')\n",
    "\n",
    "        # Bootstrapping to obtain confidence intervals.\n",
    "        scores = []\n",
    "        num_successes = 0\n",
    "        num_tries = 0\n",
    "        indices = list(range(len(ground_truth)))\n",
    "        while (num_successes < self.num_trials):\n",
    "            # Limit the number of tries.\n",
    "            num_tries += 1\n",
    "            if num_tries > 2 * self.num_trials:\n",
    "                raise ValueError(\n",
    "                    \"Too many unsuccessful tries to compute metric.\")\n",
    "\n",
    "            # Handle case where only one class is included by indices.\n",
    "            try:\n",
    "                new_indices = np.random.choice(indices, size=len(indices))\n",
    "                score = EVAL_FNS['C-STATS'](\n",
    "                    ground_truth[new_indices], predictions[new_indices],\n",
    "                    throw_error=True)\n",
    "                scores.append(score)\n",
    "                num_successes += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        scores.sort()\n",
    "        # Computed using the basic bootstrap\n",
    "        lower = 2 * mean - scores[\n",
    "            int(((1 + self.confidence_level) / 2) * num_successes)]\n",
    "        upper = 2 * mean - scores[\n",
    "            int(((1 - self.confidence_level) / 2) * num_successes)]\n",
    "        logger.log(f'Lower bound: {lower}')\n",
    "        logger.log(f'Mean: {mean}')\n",
    "        logger.log(f'Upper bound: {upper}')\n",
    "\n",
    "        cs = np.mean(cross_val_score(\n",
    "            self.model, x, y, cv=folds, scoring=scoring_fn))\n",
    "        logger.log(f'CV {self.tuning_metric}: {cs}')\n",
    "\n",
    "    def get_folds(self, x, y):\n",
    "        # TODO(ndass): try to clean this up\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        below_k = [i for i in np.argsort(counts) if counts[i] < self.k_folds]\n",
    "        shifts = {}\n",
    "        # Merge minority classes until there are more examples than folds.\n",
    "        while below_k:\n",
    "            i = below_k[0]\n",
    "\n",
    "            # Find the closest merge location on the left.\n",
    "            pos = -1\n",
    "            while (i + pos) >= 0 and counts[i + pos] == 9999999999999:\n",
    "                pos -= 1\n",
    "            left = (\n",
    "                counts[i + pos] if (i + pos) >= 0 else float(\"inf\"), i + pos)\n",
    "\n",
    "            # Find the closest merge location on the right.\n",
    "            pos = 1\n",
    "            while (i + pos) < len(values) and counts[i + pos] == 9999999999999:\n",
    "                pos += 1\n",
    "            right = (\n",
    "                counts[i + pos] if (i + pos) < len(values) else float(\"inf\"),\n",
    "                i + pos)\n",
    "\n",
    "            # Merge the current minority class with the class on the left or\n",
    "            # right that has the fewest occurences.\n",
    "            shift = min(left, right)[1]\n",
    "            counts[shift] += counts[i]\n",
    "            counts[i] = 9999999999999\n",
    "            if values[i] in shifts.values():\n",
    "                for key in shifts:\n",
    "                    if shifts[key] == values[i]:\n",
    "                        shifts[key] = values[shift]\n",
    "            shifts[values[i]] = values[shift]\n",
    "\n",
    "            # Recompute the classes that occur less than fold times.\n",
    "            below_k = [\n",
    "                i for i in np.argsort(counts) if counts[i] < self.k_folds]\n",
    "\n",
    "        # Update the minority classes of y.\n",
    "        values = list(values)\n",
    "        y_temp = np.copy(y)\n",
    "        for i in range(len(y_temp)):\n",
    "            if counts[values.index(y_temp[i])] == 9999999999999:\n",
    "                y_temp[i] = shifts[y_temp[i]]\n",
    "        _, counts = np.unique(y_temp, return_counts=True)\n",
    "        assert not [i for i in np.argsort(counts) if counts[i] < self.k_folds]\n",
    "\n",
    "        # Run StratifiedKFold on the modified version of y to get proper\n",
    "        # indices.\n",
    "        return list(StratifiedKFold(self.k_folds).split(x, y_temp))\n",
    "\n",
    "    def refit_on_full(self, train_x, train_y):\n",
    "        \"\"\"Refit model on full data.\n",
    "\n",
    "        This is to avoid using refit=True flag on GridSearchCV.\n",
    "        Before testing, we refit the best model on train+dev.\n",
    "\n",
    "        Arguments:\n",
    "            train_x: Input data to predict labels for\n",
    "            train_y: Input labels\n",
    "        \"\"\"\n",
    "        self.model.fit(train_x, train_y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict labels for the inputs.\n",
    "\n",
    "        Args:\n",
    "            x (list[list[int]]): Input data to predict labels for\n",
    "\n",
    "        Returns:\n",
    "            (list[int]): The predicted labels for the input data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Subclass of BaseModel must implement '\n",
    "                                  'predict.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnModel(BaseModel):\n",
    "    \"\"\"Abstract sklearn model class.\"\"\"\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"See BaseModel.predict.\"\"\"\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBMRegressor(SklearnModel):\n",
    "    \"\"\"LightGBM regressor.\"\"\"\n",
    "\n",
    "    def __init__(self, k_folds, tuning_metric, num_trials, confidence_level,\n",
    "                 **kwargs):\n",
    "        super(LGBMRegressor, self).__init__(\n",
    "            k_folds=k_folds, tuning_metric=tuning_metric,\n",
    "            num_trials=num_trials, confidence_level=confidence_level)\n",
    "\n",
    "        self.model_fn = lgb.LGBMRegressor\n",
    "\n",
    "        # Set the hyperparameters to sweep with grid search.\n",
    "        self.tuning_parameters = [{\n",
    "            'min_child_samples': [0, 10, 20],\n",
    "            # 'class_weight':['balanced'],\n",
    "            # 'max_bin': [4, 64, 128],\n",
    "            'max_depth': [2, 4, 8],\n",
    "            'num_leaves': [3, 7, 15],\n",
    "            # 'min_split_gain': [0, 0.1],\n",
    "            'min_child_weight': [0],\n",
    "            # 'reg_alpha': [0.],\n",
    "            # 'reg_lambda': [0., 0.01],\n",
    "            'n_estimators': [5, 10, 100],\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBMClassifier(SklearnModel):\n",
    "    \"\"\"LightGBM classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, k_folds, tuning_metric, num_trials, confidence_level,\n",
    "                 **kwargs):\n",
    "        super(LGBMClassifier, self).__init__(\n",
    "            k_folds=k_folds, tuning_metric=tuning_metric,\n",
    "            num_trials=num_trials, confidence_level=confidence_level)\n",
    "\n",
    "        self.model_fn = lgb.LGBMClassifier\n",
    "\n",
    "        # Set the hyperparameters to sweep with grid search.\n",
    "        self.tuning_parameters = [{\n",
    "            'class_weight':['balanced'],\n",
    "            #'learning_rate': [0.01, 0.1],\n",
    "            # 'num_leaves': list(7 * 2**exp for exp in range(2, 7)),\n",
    "            #'min_data_in_leaf': [25, 50, 100, 150],\n",
    "            # 'max_depth': [1, 2, 3, 4, 5, 10],\n",
    "            # 'n_estimators': [3, 5, 10, 30]\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
